#import "../generic.typ": *

#import "@preview/cetz:0.4.2"
#import "@preview/cetz-plot:0.1.3"
#import "@preview/fletcher:0.5.8" as fletcher: diagram, node, edge

= Spectrum Estimation

Estimation of the power spectrum is clearly a very useful tool. For example, under the assumption of additive noise and the signal being uncorrelated with the noise, a non-causal Wiener smoothing filter has the following frequency response:

$
H(e^(j omega)) = (P_(d x) (e^(j omega))) / (P_x (e^(j omega))) = (P_d (e^(j omega))) / (P_d (e^(j omega)) + P_v (e^(j omega)))
$

If the power spectral densities of the target signal and the noise are known, the frequency response can be calculated directly; if unknown, the result can be obtained through spectrum estimation. Other examples include the detection and tracking of narrow-band signals.

In short, this section considers the estimation of the power spectral density (PSD) of wide-sense stationary (WSS) stochastic processes. Specifically, given a random signal generated by a stochastic process, how can we effectively estimate the spectrum of that process?

The most direct idea comes from the Wiener–Khinchin Theorem: we know that the power spectrum can be obtained by the Fourier transform of the autocorrelation function:

$
P_x (e^(j omega)) = sum_(k=-infinity)^infinity r_x (k) e^(j k omega)
$

For a random signal $x[n]$ generated by an ergodic stochastic process, we can obtain the autocorrelation function of the process as follows:

$
r_x (k) = lim_(N->infinity) {1/(2N+1) sum_(n=-N)^N x[n+k] x^*[n]}
$

This is because, under the ergodicity assumption, we can unbiasedly estimate the statistical characteristics of the process using a single infinitely long realization.

However, this approach clearly has some issues: first, the samples we possess are often finite in length—for example, signals like seismic waves are inherently short, and signals like speech only approximately satisfy the stationarity assumption over very short durations; second, signal samples themselves often contain noise.

Methods for spectrum estimation can be divided into two categories: one is the Nonparametric approach, which starts by estimating the autocorrelation of the sequence and then transforms it to obtain the power spectrum; the other is the Parametric approach, usable when there is prior knowledge of the stochastic process model, which starts by estimating the model parameters and then calculates the power spectrum from the model.

== Nonparametric Spectrum Estimation

=== Periodogram <sec:se_nonparam_periodogram>

The beginning of this chapter mentioned the method of estimating the autocorrelation function from samples and then performing a Fourier transform to obtain the power spectrum. Even if the sample length is finite (e.g., $N$), we use these available samples to directly estimate the autocorrelation function:

$
hat(r)_x (k) = 1/N sum_(n=0)^(N-1-k) x[n+k] x^*[n], quad k=0, 1, dots, N-1
$

This effectively assumes that the values of $x[n]$ outside $[0, N-1]$ are all 0, which is equivalent to setting the summation range from $0$ to $N-1-k$, meaning there are $N-k$ terms in the sum. However, since we divide by $N$, the result is a biased estimation:

$
E{hat(r)_x (k)} &= 1/N sum_(n=0)^(N-1-k) E{x[n+k] x^*[n]} \
&= 1/N sum_(n=0)^(N-1-k) r_x (k) \
&= (N-k)/N r_x (k), quad k=0, 1, dots, N-1
$

As $N$ approaches infinity, the expectation equals the actual value, so it is Asymptotically Unbiased. The parts for negative $k$ can be obtained by flipping the result using the conjugate symmetry property of WSS autocorrelation functions, so it can be rewritten as:

$
E{hat(r)_x (k)} = w_B (k) r_x (k)
$ <equ:se_E_hatrxk_bw_form>

Where:

$
w_B (k) = cases(
    (N-abs(k))/N\, quad &abs(k)<=N,
    0\, &abs(k)>N
)
$

This is a Bartlett (triangular) window.

// #blockquote[
//     #text(fill: red, "(TODO)") Why not directly change the divisor to $N-k$ to get an unbiased estimate?

//     Perhaps it relates to Parseval consistency:

//     $
//     1 / (2 pi) integral_(-pi)^pi hat(P) (omega) dif omega = hat(r)_x (0) = 1 / N sum_(n=0)^(N-1) abs(x[n])^2
//     $

//     Additionally, there seem to be some circular definition issues here; let's not dwell on them for now.

//     #text(fill: red, "(TODO)") There is also the consideration of reducing variance. Furthermore, in parametric estimation later, it will be found that using the unbiased estimate (dividing by $N-k$) prevents the covariance matrix from being guaranteed positive definite. So, the poor variance performance is not just a quantitative change but a qualitative one?
// ]

Next, we use the Fourier transform to estimate the power spectrum:

$
hat(P) (e^(j omega)) = sum_(k=-N+1)^(N-1) hat(r)_x (k) e^(-j k omega)
$

The two-step process above primarily follows the definition. Once the principle is understood, we can simplify it to obtain the spectral estimate directly from the random signal $x[n]$. First, the assumption that $x[n] = 0$ outside $[0, N-1]$ can be replaced by the process of multiplying by a Dirichlet kernel (@equ:fun_dsp_sa_dirichlet_kernel), i.e., $x_N [n] = x[n] w_R [n]$, whose Fourier transform is:

#emphasis_equbox([
$
X_N (e^(j omega)) = sum_(n=-infinity)^infinity x_N [n] e^(-j omega n) = sum_(n=0)^(N-1) x[n] e^(-j omega n)
$
])

Since:

$
hat(r)_x (k) = 1/N sum_(n=-infinity)^infinity x_N [n+k] x_N^* [n] = 1/N x_N [k] * x_N^* [-k]
$

From the properties of the DTFT, the power spectrum estimate is obtained as:

#emphasis_equbox([
$
hat(P)_"per" (e^(j omega)) = 1/N X_N (e^(j omega)) X_N^* (e^(j omega)) = 1/N abs(X_N (e^(j omega)))^2
$ <equ:se_periodogram_estimator_from_X_N>
])

A power spectrum estimate satisfying this definition is called a *periodogram*.

==== An Equivalent Perspective from a Filter Bank <sec:se_periodogram_filter_bank_opinion>

The previous derivation was based on the Wiener–Khinchin theorem, following the logic of "estimate autocorrelation function $->$ Fourier transform to get spectral estimate". Next, we interpret the meaning of the periodogram from a different perspective.

An intuitive idea is that if we know the power value of each frequency component of a signal, we can *directly assemble its power spectral density plot*. So, how do we find the power of a specific frequency component? Naturally, we can first use a very narrow band-pass filter to extract that frequency component and then find a way to obtain its power; Parseval's theorem can be used here, as seen later.

Specifically, we define a set of FIR filters of length $N$ as follows:

$
h_i [n] = 1/N e^(j n omega_i) w_R [n] = cases(
    1/N e^(j n omega_i)\, quad &0<=n<N,
    0\, &"otherwise"
)
$

The Fourier transform of these filters is:

$
H_i (e^(j omega)) = sum_(n=0)^(N-1) h_i [n] e^(-j n omega) = e^(-j (omega - omega_i) (N-1)\/2) sin(N (omega - omega_i)\/2)/(N sin((omega - omega_i)\/2))
$

The bandwidth of its main lobe is approximately $Delta omega = (2 pi)/N$.

#blockquote[
    The reason for designing the filter this way is that the filter's unit impulse response $h_i [n]$ convolves with $x[n]$ in the time domain, which corresponds to the product of their Fourier transforms $H_i (e^(j omega))$ and $X(e^(j omega))$ in the frequency domain. We hope that after multiplication in the frequency domain, only the component of $X(e^(j omega))$ at frequency $omega_i$ remains, with all others being zero—effectively sampling it. Thus, ideally, $H_i (e^(j omega))$ should be $delta(omega - omega_i)$, whose time-domain expression is $1/(2 pi) e^(j n omega_i)$.

    Furthermore, since our sequence length is finite, we cannot implement an ideal $h_i [n]$ and must truncate it at length $N$. Here, we set the coefficient before $e^(j n omega_i)$ to $1\/N$ to ensure $abs(H_i (e^(j omega)))_(omega = omega_i) = 1$, simplifying calculations.
]

Note that we are now examining the true power spectral density $P_x (e^(j omega_i))$ of the stochastic process. Therefore, all instances of $x[n]$ below refer to the *stochastic process* behind the signal, examining its overall behavior after filtering rather than a specific signal.

Thus, filtering $x[n]$ through $h_i [n]$ results in another stochastic process $y_i [n]$:

$
y_i [n] = x[n] * h_i [n] = sum_(k=n-N+1)^n x[k] h_i [n-k] = 1/N sum_(k=n-N+1)^n x[k] e^(j (n-k) omega_i)
$

Since $abs(H_i (e^(j omega)))_(omega = omega_i) = 1$, at the frequency point $omega_i$, the power spectral density values of the input $x[n]$ and the output $y_i [n]$ should be identical:

$
P_x (e^(j omega_i)) = P_y (e^(j omega_i))
$

Next, we need to find this $P_y (e^(j omega_i))$ to obtain the desired $P_x (e^(j omega_i))$. This is done by considering Parseval's theorem:

$
E{abs(y_i [n])^2} = 1/(2 pi) integral_(-pi)^pi P_y (e^(j omega)) abs(H_i (e^(j omega)))^2 dif omega
$

If the bandwidth of $H_i (e^(j omega))$ is narrow enough and the sidelobes are small enough to approximate an ideal band-pass filter, we can assume that $P_y (e^(j omega)) = P_y (e^(j omega_i))$ is uniform within the passband and zero within the stopband. Thus:

$
E{abs(y_i [n])^2} approx 1/(2 pi) (Delta omega dot P_y (e^(j omega_i))) = 1/N P_y (e^(j omega_i)) = 1/N P_x (e^(j omega_i))
$ <equ:se_filter_bank_estimate_psd_from_power_in_y>

This conclusion shows that we can use the power of $y_i [n]$ to estimate $P_x (e^(j omega_i))$:

$
P_x (e^(j omega_i)) approx N E{abs(y_i [n])^2}
$

Now, since $y_i [n]$ represents a stochastic process here, we cannot examine its specific values. The problem becomes how to estimate this power $hat(E){abs(y_i [n])^2}$ from actual samples.

At this stage, we have *many choices*, but to derive the periodogram, we use a one-point average to estimate it:

$
hat(E){abs(y_i [n])^2} = abs(y_i [N-1])^2 = 1/(N^2) abs(sum_(k=0)^(N-1) x[k] e^(-j k omega_i))^2
$

This is actually an extreme and imprecise approximation. Even though a one-point average is an unbiased estimate of power, it is far too susceptible to fluctuations in the actual signal values. *However, it can be said that this is merely a way to arrive at the definition of the periodogram, providing one mode of interpretation.* Using this estimation method, we finally obtain:

$
hat(P)_x (e^(j omega_i)) = N abs(y_i [N-1])^2 = 1/N abs(sum_(k=0)^(N-1) x[k] e^(-j k omega_i))^2
$

This is consistent with the definition of the periodogram in @equ:se_periodogram_estimator_from_X_N.

In @sec:se_nonparam_mvse later, we will use a filter bank approach similar to this one, but based on @equ:fun_rp_filtering_power_in_y_repr_in_h_and_Rx, we use the autocorrelation matrix and filter coefficients to obtain a better power estimate.

==== Performance of the Periodogram

We now evaluate the performance of the periodogram as a power spectrum estimate. First, we certainly hope that the periodogram calculated from samples converges to the actual power spectrum of the stochastic process. Due to its random nature, we must consider convergence in a statistical sense, such as mean-square convergence:

$
lim_(N->infinity) E{[hat(P)_"per" (e^(j omega)) - P_x (e^(j omega))]^2} = 0
$

To satisfy this, we need its mean to be asymptotically unbiased and its variance to approach zero as the sample size becomes sufficiently large:

$
lim_(N->infinity) E{hat(P)_"per" (e^(j omega))} = P_x (e^(j omega)) \
lim_(N->infinity) "Var"{hat(P)_"per" (e^(j omega))} = 0
$

In other words, we want the periodogram to be a Consistent Estimate of the power spectral density. *To state the conclusion first*: the first condition holds, but the second does not.

Specifically, consider the *first condition*, asymptotic unbiasedness. Starting from the sample autocorrelation, its expectation is as shown in @equ:se_E_hatrxk_bw_form. From the derivation of the periodogram, we obtain:

$
E{hat(P)_"per" (e^(j omega))} &= E{sum_(k=-N+1)^(N-1) hat(r)_x (k) e^(-j k omega)}
= sum_(k=-N+1)^(N-1) E{hat(r)_x (k)} e^(-j k omega) \
&= sum_(k=-N+1)^(N-1) w_B (k) r_x (k) e^(-j k omega)
$

This is the Fourier transform of the product of the autocorrelation function and a window function. Thus, from the frequency-domain convolution property:

#emphasis_equbox([
$
E{hat(P)_"per" (e^(j omega))} = 1/(2 pi) P_x (e^(j omega)) * W_B (e^(j omega))
$ <equ:se_periodogram_expectation>
])

Where the frequency-domain expression for the Bartlett window is:

$
W_B (e^(j omega)) = 1/N [sin(N omega\/2)/sin(omega\/2)]^2
$

#blockquote[
    Note that the window function here is applied to the autocorrelation sequence, so it is called a Lag window, to distinguish it from the Data window applied directly to the data later. This is a conceptual distinction with no practical significance.
]

Note that as $N -> infinity$, $W_B (e^(j omega))$ converges to an impulse function (with a periodic integral of $2 pi$ and area concentrated at the origin, i.e., $2 pi delta(omega)$; proof omitted). Thus, the periodogram satisfies the condition for asymptotic unbiasedness.

From the perspective of spectral plots, what was originally an assembly of ideal impulse functions to form the entire spectrum now becomes an assembly using the Bartlett window function. The more samples, the closer the window function is to an ideal impulse signal, and the more accurate the spectral estimate, as shown in @fig:se_bartlett_window_freq.

#figure(
    caption: [The Fourier transforms of Bartlett windows]
)[
    #cetz.canvas({
        import cetz.draw: *
        import cetz-plot: plot

        let samples = 2048
        
        let func(N) = (w) => {
            let sden = calc.sin(w / 2)
            if (calc.abs(sden) < 1e-5) {
                N
            } else {
                (1.0 / N) * (calc.pow(calc.sin(N * w / 2), 2)) / (calc.pow(sden, 2))
            }
        }

        set-style(
            axes: (stroke: .5pt, tick: (stroke: .5pt)),
            legend: (stroke: none, orientation: ttb, item: (spacing: .1), scale: 80%)
        )

        plot.plot(
            size: (12, 6),
            x-tick-step: calc.pi / 6,
            x-format: plot.formats.multiple-of,
            x-label: $omega$,
            y-tick-step: 20,
            y-min: -0.2,
            y-max: 100.0,
            y-label: $W_B (e^(j omega))$,
            legend: "inner-north-east",
            {
                let domain = (-calc.pi / 3, calc.pi / 3)
                plot.add(
                    func(32),
                    domain: domain,
                    samples: samples,
                    label: $N = 32$,
                    style: (stroke: black)
                )
                plot.add(
                    func(64),
                    domain: domain,
                    samples: samples,
                    label: $N = 64$,
                    style: (stroke: blue)
                )
                plot.add(
                    func(256),
                    domain: domain,
                    samples: samples,
                    label: $N = 256$,
                    style: (stroke: purple)
                )
            }
        )
    })
] <fig:se_bartlett_window_freq>

The main lobe bandwidth of this function is approximately $(2 pi)/N$. Two impulse signals that are too close to each other may result in the overlap and merging of two peaks after convolution with the window function, making them impossible to distinguish clearly. Therefore, the resolution is defined as the $6"dB"$ bandwidth of the window function here:

#emphasis_equbox([
$
"Res"[hat(P)_"per" (e^(j omega))] = (Delta omega)_(6"dB") = 0.89 (2 pi)/N
$
])

$-6"dB"$ is approximately $0.5$, meaning that at about this position, the value where the two peaks overlap is half of the peak value. Considering the convolution process in @equ:se_periodogram_expectation, this will result in only one peak remaining after the two peaks overlap, making the result indistinguishable.

#blockquote[
    Consider a detail in actual calculation: the numerical value of $Delta omega$ given here is in the sense of measurement after the Fourier transform is normalized to the interval $[-pi, pi]$.

    Usually, our indicators are in actual spectrum units, in which case we need to divide by the sampling rate. For example, for a signal with a sampling rate of $10"kHz"$, if we specify that the resolution must reach at least $10"Hz"$, then $Delta omega$ should be $2 pi times (10"Hz")/(10"kHz")$.
]

*Second condition*: Consider whether the variance tends to zero. Since the periodogram has a second-order relationship with the samples, calculating the variance now involves the calculation of the fourth-order moment of the stochastic process, which is too complex. However, we can consider the special case where the stochastic process is Gaussian white noise with variance $sigma_x^2$. After a series of calculations (see Section 8.2.2, page 404 of the reference book), the variance in this case is found to be:

$
"Var"{hat(P)_"per" (e^(j omega))} = sigma_x^4
$

This is independent of $N$ and will not converge to zero as it grows. In fact, if we consider the general case, we have the following approximation (which is $(sigma_x^2)^2$ for the case of Gaussian white noise):

#emphasis_equbox([
$
"Var"{hat(P)_"per" (e^(j omega))} approx P_x^2 (e^(j omega))
$ <equ:se_periodogram_variance>
])

Therefore, the conclusion is that the second condition is not satisfied, meaning *the periodogram is not a consistent estimate of the power spectral density*.

=== Modified Periodogram

Naturally, we think about making some improvements. Let's set aside the previous derivations for a moment and directly consider modifying the definition, then verify the results. Looking back at the definition of the periodogram:

$
hat(P)_"per" (e^(j omega)) = 1/N abs(X_N (e^(j omega)))^2 = 1/N abs(sum_(n=-infinity)^(infinity) x[n] #text(fill: blue, $w_R [n]$) e^(-j omega n))^2
$

The formula reflects the process of performing spectral estimation after applying a Dirichlet kernel (i.e., a rectangular window, $w_R [n]$) to the original signal. An intuitive thought is: what would be the effect if a different window function were used here?

#text(fill: red, "(TODO)") Pages 408 and 409 of the book derive the expectation and variance. Note that there seems to be an inconsistency in the definition of $w_B (k)$ in the book; we use the normalized $w_B (k) = 1/N w_R (k) * w_R (-k) = sum_(n=-infinity)^infinity w_R (k) w_R (n-k)$ here to remain consistent with the previous sections, so there will be a slight coefficient difference from the book that needs to be adjusted.

We define the Modified periodogram as:

#emphasis_equbox([
$
hat(P)_M (e^(j omega)) = 1/(N U) abs(sum_(n=-infinity)^infinity x[n] w[n] e^(-j n omega))^2
$
])

Where $N$ is the length of the window function, and the constant $U$ is the power of the window function, which is the average of energy over time (it will later be explained that this is to make the modified periodogram asymptotically unbiased):

#emphasis_equbox([
$
U = 1/N sum_(n=0)^(N-1) abs(w[n])^2
$
])

==== Performance of Modified Periodogram

Similarly, we evaluate the performance of the modified periodogram. First is the Bias; from a similar derivation, we have:

#emphasis_equbox([
$
E{hat(P)_M (e^(j omega))} = #text(fill: blue, $1/(2 pi N U)$) P_x (e^(j omega)) * #text(fill: blue, $abs(W(e^(j omega)))^2$)
$ <equ:se_modified_periodogram_expectation>
])

Where $W(e^(j omega))$ is the Fourier transform of $w[n]$. From the previous setting of $U$, we have:

$
U = 1/N sum_(n=0)^(N-1) abs(w[n])^2 = 1/(2 pi N) integral_(-pi)^pi abs(W(e^(j omega)))^2 dif omega
$

That is:

$
integral_(-pi)^pi #text(fill: blue, $1/(2 pi N U) abs(W(e^(j omega)))^2$) dif omega = 1
$

This makes $E{hat(P)_M (e^(j omega))}$ tend toward the power spectral density as $N -> infinity$, making it asymptotically unbiased, which is the purpose of setting $U$ this way.

Next is the variance. Adding a data window does not help reduce variance, so just like @equ:se_periodogram_variance:

#emphasis_equbox([
$
"Var"{hat(P)_M (e^(j omega))} approx P_x^2 (e^(j omega))
$
])

That is, the modified periodogram is also not a consistent estimate of the power spectral density.

==== Trade-off between Resolution and Confusion

If it doesn't affect the bias and variance of the estimate, what exactly does adding a data window affect?

The Fourier transforms of different data windows have different shapes, mainly reflected in the Main lobe and Sidelobe. Referring to @sec:fun_dsp_sa, the former will affect the resolution of the spectral estimate, while the latter will introduce sidelobe interference and confusion.

We define resolution as the $3"dB"$ bandwidth of the data window's main lobe; a larger value indicates less clarity:

$
"Res"[hat(P)_"per" (e^(j omega))] = (Delta omega)_"3dB"
$

#blockquote[
    Note that the resolution defined when analyzing the Periodogram earlier was the $6"dB"$ bandwidth of the Bartlett window instead of $3"dB"$, but they are actually consistent.

    This is because the previous analysis was on the window applied to the autocorrelation sequence (i.e., the lag window); note that in @equ:se_periodogram_expectation, the term convolved with the power spectral density is $W_B (e^(j omega))$. In contrast, the analysis here is on the data window; note that in @equ:se_modified_periodogram_expectation, the term convolved with the power spectral density is $1/(N U) abs(W (e^(j omega)))^2$. There is a square relationship between the two.

    Therefore, the $-6"dB"$ point of the former is consistent with the $-3"dB"$ point of the latter, both being the half-power points relative to the signal.
]

The approximate values for sidelobe suppression and resolution of common window functions are summarized in @tab:se_properties_of_data_windows.

#figure(
    caption: "Properties of a few commonly used windows with length N"
)[
    #table(
        columns: 3,
        stroke: (x, y) => if y == 0 {
            (bottom: 0.7pt + black)
        },
        align: (x, y) => (
            if x > 0 { center }
            else { left }
        ),
        table.header(
            [], [Sidelobe (dB)], [Resolution]
        ),
        "Rectangular", $-13$, $0.89 (2 pi \/ N)$,
        "Bartlett", $-27$, $1.28 (2 pi \/ N)$,
        "Hanning", $-32$, $1.44 (2 pi \/ N)$,
        "Hamming", $-43$, $1.30 (2 pi \/ N)$,
        "Blackman", $-58$, $1.68 (2 pi \/ N)$,
    )
] <tab:se_properties_of_data_windows>

It can be observed that often, the better the sidelobe suppression, the worse the resolution (i.e., the larger the $3"dB"$ bandwidth). This is a Trade-off.

=== Periodogram Averaging

Thus far, none of the above methods can provide a consistent estimate of the power spectral density due to non-convergent variance. In the following, we obtain the desired consistent estimate through several methods of averaging periodograms.

Consider that previously for a random variable $x$, we obtained a consistent estimate of its mean $E{x}$ by collecting a large number of uncorrelated measurement samples and calculating the sample mean. By analogy, theoretically, we need to use multiple uncorrelated realizations of the stochastic process $x(n)$, calculate the periodogram for each, and then average them to estimate the expectation of the periodogram.

Specifically, suppose we have $K$ uncorrelated realizations $x_i [n]$, each of length $L$, with the total number of sample points being $N = L K$. Calculate the periodogram for each realization:

$
hat(P)_"per"^((i)) (e^(j omega)) = 1/L abs(sum_(n=0)^(L-1) x_i [n] e^(-j n omega))^2, quad i = 0, 1, dots, K-1
$

Then average them to get the final spectral estimate:

$
hat(P)_x (e^(j omega)) = 1/K sum_(i=0)^(K-1) hat(P)_"per"^((i)) (e^(j omega)) = 1/N sum_(i=0)^(K-1) abs(sum_(n=0)^(L-1) x_i [n] e^(-j n omega))^2
$

As usual, we evaluate its bias and variance. First, because it is just another average, the expectation is the same as before:

#emphasis_equbox([
$
E{hat(P)_x (e^(j omega))} = E{hat(P)_"per"^((i)) (e^(j omega))} = 1/(2 pi) P_x (e^(j omega)) * W_B (e^(j omega))
$
])

Thus, it is asymptotically unbiased as $L -> infinity$. Then consider the variance; since the different realizations are uncorrelated:

#emphasis_equbox([
$
"Var"{hat(P)_x (e^(j omega))} &= 1/K^2"Var"{sum_(i=0)^(K-1) hat(P)_"per"^((i)) (e^(j omega))} \
&= 1/K"Var"{hat(P)_"per"^((i)) (e^(j omega))} approx 1/K P_x^2 (e^(j omega))
$
])

In summary, using this averaging-based method can provide a *consistent estimate* of the power spectral density when both $L$ and $K$ tend to infinity.

==== Bartlett's Method

In general practical situations, we do not have that many independent realizations. However, if we have a sufficiently long realization and the underlying stochastic process satisfies the ergodicity assumption, we can cut it into small segments and use them as uncorrelated realizations to obtain a spectral estimate, known as Bartlett's method.

Let the signal length be $N$, cut into $K$ non-overlapping segments (to ensure they are uncorrelated as much as possible), each of length $L$. If we further let:

$
x_i [n] = x[n + i L], quad n = 0, 1, dots, L-1; quad i = 0, 1, dots, K-1
$

The notation becomes consistent with the previous analysis. We directly obtain the formula:

#emphasis_equbox([
$
hat(P)_B (e^(j omega)) &= 1/K sum_(i=0)^(K-1) (1/L abs(sum_(n=0)^(L-1) x[n + i L] e^(-j n omega))^2) \
&= 1/N sum_(i=0)^(K-1) abs(sum_(n=0)^(L-1) x[n + i L] e^(-j n omega))^2
$
])

*The bias and variance are consistent with the previous analysis of the averaging method.* However, even if they do not overlap, there will certainly be some correlation between the segmented sequences (ergodicity only guarantees that correlation gradually vanishes over a long time), so the variance might be slightly smaller than analyzed previously. Nevertheless, we still approximately consider the segments to be uncorrelated and take this approximate value for the variance.

Next, let's analyze the impact of this method on resolution. Since the original sequence of length $N$ is cut into small segments of length $L$, the actual sequence length used when calculating the periodogram is only $L$, so the resolution is:

$
"Res"[hat(P)_B (e^(j omega))] = 0.89 (2 pi)/L = 0.89 K (2 pi)/N
$

It can be seen that compared to calculating the periodogram using the entire sequence of length $N$, the resolution has deteriorated by a factor of $K$; this is the trade-off.

==== Welch's Method

Bartlett's method uses periodogram averaging. Next, we follow this idea but use modified periodogram averaging, known as Welch's method.

The idea of the modified periodogram is to apply a window function to the data, which can actually weaken the correlation of the signal at the edges of two adjacent segments. Therefore, we can consider relaxing the requirements and allow for *overlapping* when segmenting the data. Each segment is still of length $L$, but the starting points of each segment are only spaced by $D$ samples (overlap occurs when $D < L$):

$
x_i [n] = x[n + i D], quad n = 0, 1, dots, L-1; quad i = 0, 1, dots, K-1
$

The total number of sample points is then $N = L + D(K - 1)$. Typically, we use a $50%$ overlap, i.e., $D = L \/ 2$. The data window is applied to each segment, and the window length is consistent with the segment length $L$. Thus, we obtain the spectral estimate for Welch's method as:

#emphasis_equbox([
$
hat(P)_W (e^(j omega)) &= 1/K sum_(i=0)^(K-1) (1/(L U) abs(sum_(n=0)^(L-1) w[n] x[n + i D] e^(-j n omega))^2) \
&= 1/(K L U) sum_(i=0)^(K-1) abs(sum_(n=0)^(L-1) w[n] x[n + i D] e^(-j n omega))^2
$
])

Analyzing the indicators as usual: first, the bias is consistent with the Modified periodogram, substituting $N$ with $L$:

#emphasis_equbox([
$
E{hat(P)_W (e^(j omega))} = E{hat(P)_M^((i))} = 1/(2 pi L U) P_x (e^(j omega)) * abs(W(e^(j omega)))^2
$
])

The variance is related to the degree of overlap and is difficult to calculate; we only consider the case of $50%$ overlap:

#emphasis_equbox([
$
"Var"{hat(P)_W (e^(j omega))} approx 9/(8 K) P_x^2 (e^(j omega)) approx 9/16 L/N P_x^2 (e^(j omega))
$
])

The coefficient $9\/8$ seems to indicate that the variance performance has slightly worsened, but since the number of segments $K approx (2 N)/L$ has nearly doubled, the variance performance is actually improved.

=== Periodogram-based Methods Summary

The reference book also mentions the Blackman-Tukey method, which is omitted here. Aside from that, we have analyzed the mean, variance, resolution, and other performances of each method. A summary is provided in @tab:se_periodogram_formula_summary.

#figure(
    caption: "Properties of a few commonly used windows with length N"
)[
    #resize_box([
        #table(
            columns: 5,
            stroke: (x, y) => if y == 0 {
                (bottom: 0.7pt + black)
            },
            align: horizon,
            table.header(
                [], [Definition $hat(P)_x (e^(j omega))$], [Expectation], [Variance (approx.)], [Resolution $Delta omega$]
            ),
            "Periodogram",
            $display(1/N abs(sum_(n=-infinity)^(infinity) x[n] w_R [n] e^(-j omega n))^2)$,
            $display(1/(2 pi) P_x (e^(j omega)) * W_B (e^(j omega)))$,
            $display(P_x^2 (e^(j omega)))$,
            $display(0.89 (2 pi)/N)$,

            "Modified periodogram",
            $display(1/(N U) abs(sum_(n=-infinity)^infinity x[n] w[n] e^(-j omega n))^2)$,
            $display(1/(2 pi N U) P_x (e^(j omega)) * abs(W(e^(j omega)))^2)$,
            $display(P_x^2 (e^(j omega)))$,
            [See @tab:se_properties_of_data_windows],

            [Bartlett's \ $N = K L$],
            $display(1/N sum_(i=0)^(K-1) abs(sum_(n=0)^(L-1) x[n + i L] e^(-j omega n))^2)$,
            $display(1/(2 pi) P_x (e^(j omega)) * W_B (e^(j omega)))$,
            $display(1/K P_x^2 (e^(j omega)))$,
            $display(0.89 K (2 pi)/N)$,

            [Welch's ($50%$ overlap) \ $N = L + D(K - 1)$],
            $display(1/(K L U) sum_(i=0)^(K-1) abs(sum_(n=0)^(L-1) w[n] x[n + i D] e^(-j omega n))^2)$,
            $display(1/(2 pi L U) P_x (e^(j omega)) * abs(W(e^(j omega)))^2)$,
            $display(9/16 L/N P_x^2 (e^(j omega)))$,
            "Window dependent"
        )
    ])
] <tab:se_periodogram_formula_summary>

Note that in the modified periodogram $U = 1/N sum_(n=0)^(N-1) abs(w[n])^2$, whereas in Welch's method, since each segment is of length $L$, its $U = 1/L sum_(n=0)^(L-1) abs(w[n])^2$.

The reference book defines two indicators to measure the performance of the above methods. The first is Variability:

$
cal(V) = ("Var"{hat(P)_x (e^(j omega))})/(E^2{hat(P)_x (e^(j omega))})
$

Simply put, it is the normalized variance. The second is the Figure of merit:

$
cal(M) = cal(V) Delta omega
$

Which is the product of variability and resolution; the smaller this value, the better. Incidentally, such indicators defined as the product of two quantities generally multiply the variables involved in a trade-off. Consequently, we will find that the figures of merit for these nonparametric estimation methods are quite similar.

=== Minimum Variance (MV) Spectrum Estimation <sec:se_nonparam_mvse>

First, we expand on the idea of feeding signals into filter banks as discussed in @sec:se_periodogram_filter_bank_opinion. In 그 section, the filters were fixed and independent of the data $x[n]$, termed "data independent." In such cases, if some filters happen to pass too much energy through their sidelobes, it leads to significant interference.

The Minimum Variance (MV) Spectrum Estimation method introduced in this section aims to design a filter for each frequency point based on the input signal $x[n]$, such that each filter: 1. Has a gain of unity at the target frequency $omega_i$, passing it without loss; 2. Minimizes the energy passed through the sidelobes. This yields better estimation results.

We first define notation: let $g_i [n]$ be a $p$-th order complex-valued FIR band-pass filter. To satisfy the *first requirement*, we should have:

$
G_i (e^(j omega_i)) = sum_(n=0)^p g_i [n] e^(-j n omega_i) = 1
$

For convenience, we write this in vector form. Let $bold(g)_i = [g_i [0], g_i [1], dots, g_i [p]]^T$ and $bold(e)_i = [1, e^(j omega_i), dots, e^(j p omega_i)]^T$. The constraint becomes:

$
bold(g)_i^H bold(e)_i = bold(e)_i^H bold(g)_i = 1
$

Note that while $bold(g)_i^H bold(e)_i = (bold(e)_i^H bold(g)_i)^*$, it is set to $1$ here. Since the result is known to be real, this is acceptable, but remember this equality only holds here.

The *second requirement* involves minimizing the power of the output process. Regarding this power value, from @equ:fun_rp_filtering_power_in_y_repr_in_h_and_Rx:

$
E{abs(y_i [n])^2} = bold(g)_i^H bold(R)_x bold(g)_i
$

Thus, we want to minimize this value while satisfying the aforementioned linear constraint, i.e., solve:

$
min_(bold(g)_i) bold(g)_i^H bold(R)_x bold(g)_i quad "s.t." bold(e)_i^H bold(g)_i = 1
$

The solution to this problem is:

$
bold(g)_i = (bold(R)_x^(-1) bold(e)_i)/(bold(e)_i^H bold(R)_x^(-1) bold(e)_i) \
min_(bold(g)_i) bold(g)_i^H bold(R)_x bold(g)_i = 1/(bold(e)_i^H bold(R)_x^(-1) bold(e)_i)
$

#blockquote[
    This is a typical optimization problem that can be solved using the Lagrange multiplier method. The solution process is as follows. Let:

    $
    L(bold(g)_i, lambda) = bold(g)_i^H bold(R)_x bold(g)_i - lambda (bold(e)_i^H bold(g)_i - 1)
    $

    Setting the partial derivatives with respect to the two parameters to zero yields (since $bold(R)_x$ is Hermitian):

    $
    cases(
        2 bold(R)_x bold(g)_i - lambda bold(e)_i = 0,
        bold(e)_i^H bold(g)_i = 1
    )
    $

    From the first equation, we have $bold(g)_i = lambda/2 bold(R)_x^(-1) bold(e)_i$. Substituting this into the second equation and rearranging gives:

    $
    lambda/2 = 1/(bold(e)_i^H bold(R)_x^(-1) bold(e)_i)
    $
    
    Substituting this back into the previous equation yields the solution:

    $
    bold(g)_i = (bold(R)_x^(-1) bold(e)_i)/(bold(e)_i^H bold(R)_x^(-1) bold(e)_i)
    $
    
    Substituting this back gives the analytical expression for the minimum value.
]

Since the derivation above holds for any $omega_i$, we can directly drop the subscripts and write it as a function of $omega$:

$
hat(sigma)_x^2 (omega) = 1/(bold(e)^H bold(R)_x^(-1) bold(e))
$

Where $bold(e) = [1 quad e^(j omega) quad e^(j 2 omega) quad dots quad e^(j p omega)]^T$. The corresponding filter parameters for this process are:

#emphasis_equbox([
$
bold(g) = (bold(R)_x^(-1) bold(e))/(bold(e)^H bold(R)_x^(-1) bold(e))
$
])

However, $hat(sigma)_x^2 (omega)$ is only an estimate of the output process power; it *cannot yet be used directly as a power spectral density estimate*. We must divide by the filter bandwidth, for reasons analogous to @equ:se_filter_bank_estimate_psd_from_power_in_y. There are multiple ways to define bandwidth; we can take the simplest one, which provides correct estimation results for the white noise example (see the example on page 429 of the book):

$
Delta/(2 pi) = 1/(p+1)
$

Thus, we finally obtain the power spectral density estimate:

#emphasis_equbox([
$
hat(P)_"MV" (e^(j omega)) = (hat(sigma)_x^2 (omega))/(Delta\/2 pi) = (p+1)/(bold(e)^H bold(R)_x^(-1) bold(e))
$
])

This is called the *minimum variance spectrum estimate*. Note that it uses the autocorrelation matrix $bold(R)_x$ of the stochastic process. If we only have sample data, we need to use the estimated $hat(bold(R))_x$:

#emphasis_equbox([
$
hat(bold(R))_x = 1/K sum_(i=0)^(K-1) bold(x)_i bold(x)_i^H \
bold(x)_i = [x[i] quad x[i+1] quad x[i+2] quad dots quad x[i+L-1]]^T
$
])

Here, we segment the signal samples of length $N$ into $K$ overlapping segments, each of length $L$, with starting point intervals of only $D = 1$, i.e., $K = N - L + 1$, to estimate the sample autocorrelation matrix. *For dimension matching, the previous filter order is related to the sequence length by $L=p+1$*, yielding:

#emphasis_equbox([
$
hat(P)_"MV" (e^(j omega)) = L/(bold(e)^H bold(R)_x^(-1) bold(e))
$
])

#blockquote[
    #Cre("(TODO)") Tired. Let it be.
]

== Parametric Spectrum Estimation

=== For Autoregressive (AR) Models

#Cre("(TODO)") the Yule-Walker Method (autocorrelation method) and the covariance method.

=== Mulitple Signal Clasification (MUSIC)

#Cre("(TODO)")
