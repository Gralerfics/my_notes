#import "../generic.typ": *

#import "@preview/cetz:0.4.2"
#import "@preview/fletcher:0.5.8" as fletcher: diagram, node, edge
#import "@preview/suiji:0.4.0": *

= Linear Algebra

#text(fill: red, "（TODO）")主要关于线性方程组的解、秩、逆、特征值、二次型与正定性、矩阵微积分等。

= Probability Theory

== Random Variable

一些现象（如投骰子）因具有随机性而难以预先知道结果，我们称为*随机现象*。随机现象不断发生，我们每观察并记录一次其结果称为一次*随机试验*，记录的结果称为一个样本。

一次随机试验可能得到各种不同的结果（样本），所有样本的集合称为该随机现象的样本空间，其中包含了该随机现象可能出现的所有结果。

*随机事件*是样本空间的子集，它包含了所有可能发生的结果（样本）中的一部分结果（样本）。如果一次随机试验得到的结果（样本）$omega$ 属于某个随机事件 $A$ 包含的样本子空间，则称随机事件 $A$ 发生了，否则称没有发生。

随机事件可以是各种陈述，例如 "明天下雨" 是 "明天下雨与否" 这一随机现象的一个随机事件，明天下雨与否的样本空间是 ${"下雨", "不下雨"}$，而 "明天下雨" 就是其子集 ${"下雨"}$。又如，"投掷一个均匀二十面骰子的结果大于十八" 是 "投掷一个均匀二十面骰子所得结果" 这一随机现象的一个随机事件，随机现象的样本空间是 ${1, 2, ..., 20}$，而该随机事件可以记为 ${19, 20}$。

*随机变量* $x$ 用于描述一些结果可以数量化的随机现象的结果，例如前述 "投掷一个二十面骰子所得的结果是 $1$ 到 $20$ 共二十个整数。而 "明天下雨与否" 这种不显式包含数量的样本空间也可以通过人为规定 "下雨为 1，不下雨为 0" 来实现数量化的表达。

#blockquote[
    实际上大部分讲义中用大写字母表示随机变量，但这只是记号的区别。为了同之后随机过程中的记号统一，这里就用小写字母表示，并在必要的时候注明此为随机变量。相应的，参数等自变量（例如后面概率密度函数的参数）就用希腊字母 $alpha$ 等表示，以免混淆。
]

一个随机事件 $A$ 发生的*概率*记为 $"Pr"(A)$，表示事件发生的可能性。包含样本空间全集的随机事件的概率为 $1$，因为随机试验的结果一定被包含在其中，表示该随机事件必然发生。其他关于概率、条件概率、全概率和样本空间集合的内容就不再赘述。

== Probability Density Function (PDF)

一个随机变量的不同取值具有不同的概率，如果考虑离散随机变量（可能取的值的个数是有限的），那么一个描述其所有可能取值到各取值概率的映射称为概率质量函数（Probability Mass Function，PMF），它包含了关于这个分布的所有信息。

如果考虑连续随机变量，可能取值的个数是无限的，那么取到某个具体值的概率都近似为 $0$，不能再用类似 PMF 的方式为每个取值赋予概率值的方式来构建函数了。我们首先为随机变量 $x$ 定义累计分布函数（Cumulative Distribution Function，CDF）：

$ F_x (alpha) = "Pr"{x <= alpha} $

大括号或是小括号只是记号，不影响。总之它的意思是 $x$ 小于或等于参数 $alpha$ 的概率。显然，对于取值为实数的连续随机变量 $x$，$alpha$ 在负无穷时 CDF 趋于 $0$，$alpha$ 在正无穷时 CDF 趋于 $1$。我们通过 CDF 定义概率密度函数（Probability Density Function，PDF）：

$ f_x (alpha) = dif / (dif alpha) F_x (alpha) $

即 PDF 是 CDF 的微分，由微积分基本定理可以知道，对 PDF 从 $alpha_1$ 到 $alpha_2$ 的积分（即这一段曲线下的面积）就等于随机变量 $x$ 取值落在这一段上的概率：

$
integral_(alpha_1)^(alpha_2) f_x (alpha) dif alpha &= F_x (alpha_2) - F_x (alpha_1) \ &= "Pr"{x <= alpha_2} - "Pr"{x <= alpha_1} \ &= "Pr"{alpha_1 < x <= alpha_2}
$

同样地，PDF 包含了一个随机变量的分布的所有信息。

== Joint Distribution

联合分布是关于多个随机变量协同分布关系的描述。例如对于两个随机变量 $x$ 和 $y$，定义联合累积密度函数为：

$ F_(x, y) (alpha, beta) = "Pr"{x <= alpha, y <= beta} $

非常直白，就是同时对两个随机变量做出限制。其联合概率密度函数为：

$ f_(x, y) (alpha, beta) = partial^2 / (partial alpha partial beta) F_(x, y) (alpha, beta) $

== Mathematic Expectation

数学期望可以认为是进行无穷多次随机试验后，将所得结果取平均后的值。从定义上讲，是所有可能结果用其概率加权的平均值，例如连续随机变量 $x$ 的期望是：

$ E(x) = integral_(-infinity)^infinity alpha f_x (alpha) dif alpha $

若是离散随机变量则更清晰一些（$alpha$ 为任意可能取值）：

$ E(x) = sum_(alpha) alpha f_x (alpha) $

很简单，但举一个不是求某个随机变量期望的例子，如随机变量 $x$ 的方差被定义为 $E{(x - E(x))^2}$，其积分式应为：

$ E{(x - E(x))^2} = integral_(-infinity)^infinity (alpha - E(x))^2 f_x (alpha) dif alpha $

这里用的分布函数仍然是 $f_x (alpha)$ 而不是类似 $f_x ((alpha - E(x))^2)$ 或者 $f_((x - E(x))^2) (alpha)$ 的什么东西——我想表达的是，我们应该给 $E$ 加一个下标，表示是对哪个随机变量的分布求期望：

$ E_x (x) = integral_(-infinity)^infinity alpha f_x (alpha) dif alpha $

因为期望的求解是需要两个要素的：一是分布，平均的过程是在这个分布上进行的；二是表达式，也就是括号里的东西。期望的定义式中，分布函数 $f_x (alpha)$ 下面的 $x$ 是来自于 $E_x$ 的 $x$，代表的是第一要素的分布，而不是 $E(x)$ 括号里表达式的 $x$；而分布函数前乘的表示取值的 $alpha$，是将参数 $alpha$ 带入表达式中的 $x$ 后得到的表达式，详见例子 @equ:joint_expectation_example。

这其实是一个简单的问题，但有时没转过弯就可能在这里卡一下，因为期望的符号中对分布的描述往往是被省略了，默认认为是表达式中存在的随机变量的分布，这里仅作辨析提醒。

对于存在多个随机变量的情况，我们更需要明确到底是求什么分布下的期望，例如关于两个随机变量的表达式的期望，积分应改为二重积分以遍历所有取值组合，分布函数也应改为联合概率密度函数，例如：

$ E_(x, y) (x y^*) = integral_(alpha=-infinity)^infinity integral_(beta=-infinity)^infinity alpha beta^* f_(x, y) (alpha, beta) dif alpha dif beta $ <equ:joint_expectation_example>

== Moments, Mean and Variance

接下来就是基于上述内容纯定义性质的一些统计量了。

首先定义随机变量 $x$ 的 *$k$ 阶原点矩*为 $E(x^k)$，而一阶原点矩就是 $x$ 的均值（Mean），记为 $m_x = E(x)$。

再定义 *$k$ 阶中心矩*为 $E{(x - E(x))^k}$，可见二阶中心矩就是 $x$ 的方差，记为 $sigma^2_x = "Var"(x)$，$sigma_x$ 称为标准差，为方差的平方根。

其他一些统计量及其对应的物理含义就不赘述了。

== Independence

独立性描述两个随机变量是否互不影响。如果两个随机变量 $x$ 和 $y$ 独立，那就说明 $x$ 取某个值 $alpha$ 的概率乘以 $y$ 取某个值 $beta$ 的概率相乘就能直接得到它们分别取这两个值的概率，这是因为独立事件的概率相乘等于它们同时发生的概率，参考乘法原理。

对所有可能取值都满足这一条件，也就意味着二者的分布相乘就可以直接得到联合分布，即独立性的充要条件：

$ f_(x, y) (alpha, beta) = f_x (alpha) f_y (beta) $

== Covariance Function

协方差（Covariance）用于衡量随机变量之间的相关程度，两个随机变量 $x$ 和 $y$ 的协方差定义为：

$ c_(x y) = "Cov"(x, y) = E{(x - m_x)(y - m_y)^*} $

顺便，注意到 $c_(x x) = E{(x - m_x)^2} $ 就是 $x$ 的方差。

可以认为，协方差衡量的是随机变量之间的线性关系显著程度。具体地，从定义上看它可以理解为：当 $x$ 高于均值时，$y$ 倾向于同时高于（或低于）自己均值的程度。这个程度越大，通俗地说就是说明 $x$ 越大时 $y$ 也倾向于越大（或越小），二者的线性关系就较强。因为减去均值免去了其影响，它衡量的就主要是变量的相对变化趋势。

== Correlation Function

相关函数（Correlation Function）定义为两个随机变量内积的期望：

$ r_(x y) = E{x y^*} $

它可以看作未去除均值的协方差函数。可以推导二者关系为：

$
c_(x y) &= E{(x - m_x)(y - m_y)^*} \
&= E{x y^*} + E{m_x m_y^*} - m_x E{y^*} - E{x} m_y^* \
&= E{x y^*} + m_x m_y^* - m_x m_y^* - m_x m_y^* \
&= r_(x y) - m_x m_y^*
$

它们之间只是相差一个常数 $m_x m_y^*$，所以它们表达的物理意义可以是类似的。

== Correlation Coefficient and Orthogonality

当然，协方差没有去除量纲，在不同的随机变量组合中不具备普遍性。通常我们利用归一化等手段定义相关系数来衡量相关性（Correlation），例如最常用的皮尔逊相关系数：

$ rho_(x y) = c_(x y) / (sigma_x sigma_y) $

可以证明其取值范围为 $[-1, 1]$，用以统一地衡量随机变量间线性关系的显著程度。这个相关系数如果为零，就称两个随机分布是正交（Orthogonal）的。

当相关系数为 $0$ 时，也即 $c_(x y) = 0$ 时，代表两个随机变量不相关。相关系数为正时代表正相关，为负时代表负相关。可以推得不相关的充要条件：

$
c_(x y) = 0  quad => quad r_(x y) - m_x m_y^* = 0 quad => quad E{x y^*} = E{x} E^*{y}
$

即随机变量相乘的期望等于随机变量的期望相乘。

#blockquote[
    注意，相关性和独立性不是一回事。
    
    首先，不相关不一定代表独立。例如，两个随机变量满足 $y = x^2$ 时，虽然二者完全不独立，但相关系数仍然是 $0$。如前所述，相关性主要衡量的是线性关系的显著程度，在这个例子中，$x$ 为负时 $y$ 随 $x$ 增长而减小，$x$ 为正时 $y$ 随 $x$ 增长而增长，对称地抵消了两部分关系，结果上看没有线性关系的成分，但其实存在二次关系。

    但独立就一定不相关，因为独立时两个随机变量的分布完全没有关联，各自的密度函数直接相乘就能得到联合分布密度函数，由定义也可推得 $E{x y^*} = E{x} E^*{y}$。

    总结来说就是，$"独立" => "不相关"$，不是充要关系。
]

= Random Process

一个随机过程 ${x(n)}$ 实际就是一串随机变量拼成的序列。

这些随机变量之间可以是独立同分布（i.i.d）的，例如白噪声，但也可以不是，或者说现实世界里大部分都不是；这个序列的索引可以代表时间，也可以不是，方便起见我们后面默认在研究时间序列，索引 $n$ 代表不同的时间点。

同前，对随机性的研究重点在于研究变化中不变的东西，如数据的统计特征，故我们还是从提供一系列统计特征的定义开始。

== Statistic <sec:fun_rp_statistic>

=== Mean

不是说随机过程会有一个统一的均值，这个均值还是分别定义给每个随机变量的，得到的是一个序列：

$ m_x (n) = E{x(n)} $

=== Auto-correlation <sec:fun_rp_autocorrelation>

对于一个随机过程 $x(n)$，其自相关函数即为指定两个索引 $k$ 和 $l$ 的随机变量的相关函数：

$ r_x (k, l) = r_(x(k), x(l)) = E{x(k) x^*(l)} $ <equ:fun_rp_autocor_def>

就该公式来看，这里的每一个 $r_x$ 只是和两个随机变量有关，和随机过程整体没有什么关联，不过后续介绍宽平稳（WSS）的时候就有别的说法了。

=== Auto-covariance

类比协方差定义自协方差：

$ c_x (k, l) = E{[x(k) - m_x (k)] [x(l) - m_x (l)]^*} = r_x (k, l) - m_x (k) m_x^* (l) $

=== Cross-correlation

互相关则是关于两个随机过程的，例如 ${x(n)}$ 和 ${y(n)}$ 的互相关函数定义为：

$ r_(x y) (k, l) = r_(x(k), y(l)) = E{x(k) y^*(l)} $

=== Cross-covariance

同样地，互协方差定义为：

$ c_(x y) (k, l) = E{[x(k) - m_x (k)] [y(l) - m_y (l)]^*} = r_(x y) (k, l) - m_x (k) m_y^* (l) $

这些后面其实用不太到，主要是列举一下。

== Structural Invariance

可以注意到，前面定义的统计量主要还是关注分立的随机变量，而与随机过程的整体没有太大关联；而接下来我们要来关注整个随机过程的结构上的一些不变性，这些性质将会在分布未知而只能通过有限的样本进行估计时提供合理的依据。

具体地，随机变量是理想的，拥有一个概率密度函数来描述它的分布；但我们往往研究具体的信号而不知其真实分布，只能借助样本反过来估计真实分布的情况。我们可以将一个具体的信号 $x[n]$ 视为一个随机过程的一次实现（Realization），其每个时间点的样本都来自对这个随机过程中对应时间随机变量的一次采样。@fig:fun_rp_realizations_of_rp 展示了对一个随机过程进行大量试验后得到的一系列可能实现，高亮的为其中一条。

#figure(
    caption: [Different realizations of a random process]
)[
    #cetz.canvas({
        import cetz.draw: *

        let samples = 100
        let len = 30

        let (x, y) = (0, 0)
        let (x-new, y-new) = (0, 0)
        let v = ()
        let col = black
        
        // let rng = gen-rng-f(datetime.today().day())
        let rng = gen-rng-f(42)

        for idx in range(samples) {
            (rng, v) = uniform-f(rng, low: -1.0, high: 1.0, size: 1)
            (x, y) = (0, v.at(0))
            
            for k in range(len) {
                (rng, v) = uniform-f(rng, low: -0.4, high: 0.15, size: 1)
                (x-new, y-new) = (x + 0.4, y + v.at(0))


                if (idx == 0) {
                    col = color.mix(
                        (blue.transparentize(0%), 1 - k / len),
                        (green.transparentize(0%), k / len)
                    )
                } else {
                    col = color.mix(
                        (blue.transparentize(90%), 1 - k / len),
                        (green.transparentize(90%), k / len)
                    )
                }
                
                line(stroke: (paint: col, cap: "round", thickness: 1pt),
                    (x + 0.025, y), (x-new, y-new)
                )

                (x, y) = (x-new, y-new)
            }
        }
    })
] <fig:fun_rp_realizations_of_rp>

=== Stationarity

我们关注的*第一个问题*是：在一个随机过程中，一些统计特征会不会随时间而改变？

我们引入平稳性（Stationarity）的概念，主要思想是衡量任意一段子序列在延迟任意一段时间后，其统计特征是否还维持不变。要使统计特征一致，最简单的就是直接令分布完全一致，由此定义强平稳（Strict-Sense Stationary，SSS）：

$ forall k, {n_1, n_2, dots, n_m}, f_(x(n_1), x(n_2), dots, x(n_m)) (dot) = f_(x(n_1 + k), x(n_2 + k), dots, x(n_m + k)) (dot) $

强平稳的要求太高，大部分情况下我们也不需要如此强的条件，故我们定义宽平稳（Wide-Sense Stationary，WSS），只关注一阶和二阶统计量的一致性：

#align(center)[
    + $ m_x (n) = m_x $
    + $ r_x (k, l) = r_x (k - l) $
    + $ c_x (0) < infinity $
]

即对于宽平稳过程：均值与时间无关，自相关函数只与时间差有关而与绝对时间无关。

#blockquote[
    上述条件的第三条，即方差有限，在许多材料中会被省略。

    方差有限等价于均方值有限，二者就差一个均值的平方，物理意义上可以认为它代表功率有限，而工程领域常忽略这一条件是由于大部分实际物理过程功率都有限。
    
    数学上，这一条确保了二阶矩的存在。不过实际上第二条件中要求 $r_x (0)$ 存在，而期望的存在性隐含了其收敛到有限值的含义，已经隐含了第三条件。
]

这使得我们在一条实现上截取不同时间的子序列，其平均性质是一致的。在信号平稳的情况下，我们可以将一条够长的样本劈开劈成几份，并声称这劈出的几份背后隐含着一致的统计特征。

=== Ergodicity

第一个问题引申出的平稳性让我们可以去劈开一条足够长的样本当作不同的样本来用，那么我们关注的*第二个问题*是：一条足够长的实现能否代表无穷多条可能实现的总体（样本空间）？也就是我拿这条长样本劈开来去估计分布的，但它估计出来的是我要的这个随机过程的分布吗？它一条样本能代表这个随机过程包含的所有方面的信息吗？

我们用遍历性（Ergodicity）来描述这个性质，它是建立在平稳性的基础上的，即要保证统计特征不随时间推移变化，否则每个部分性质不一样，有足够长的样本也是白搭。

首先，我们需要举一个 "平稳但不遍历" 的典型例子来说明这个概念存在的必要性。令随机过程 $x(n)$ 中仅 $x(0)$ 是一个一次性抽取的随机变量 $z$，之后都有 $x(n) = x(n-1)$。这个例子称为随机常数，这是由于该随机过程的所有实现都是一条常值信号，但不同实现中这个常数值不一样，取决于一开始随机的数值。

分析这个过程，首先它一定是平稳的，因为时间变化显然不影响其分布。也可以具体一些证明其满足 WSS 的条件：考察均值，$m_x = E{x(0)}$ 与 $n$ 无关；考察自相关函数 $r_x (k, l) = E{x(k) x^*(l)} = E{x(0) x^*(0)} = sigma_x^2$，与什么都无关。

然后考察遍历性，注意到样本总体的均值为 $E{z}$，但对于一个足够长的实现，其时间平均为该实现中 $x(0)$ 抽到的值，时间再长也是这个值，而这个值不一定等于 $E{z}$。换句话说，我们没有办法在这种情况下仅通过一条实现来估计出 $z$ 的性质，即便它足够长。

*总结来说*，平稳性保证了我们可以用一条足够长的样本得到有意义的时间平均，而遍历性则保证了这个时间平均等于所有可能样本的总体平均，即这样估出来的值是正确的。

#blockquote[
    物理学和统计力学中的相同概念往往翻译为 "各态历经性"，这其实更为形象。它表示随着时间的推移，系统将会经历并表现出其所有可能的宏观状态。只有单次实现中有表现出分布所有方面特征的可能性，我们才有机会用单条样本来估计宏观分布。

    我们也可以从偏估计和无偏估计的角度去看待这个问题。对于平稳信号我们可以去估计出一个有意义的统计量，但并不能保证估计是否无偏，此时满足遍历性就可以保证这一点。
]

严格的遍历性定义比较抽象和复杂，所以类似 WSS 的定义，我们也仅考虑在均值和相关性上考察遍历性，就够满足我们的需要了。当然，以下讨论建立在至少是 WSS 的基础上。

首先是*均值遍历性*（Mean-Ergodicity），我们定义样本均值，也就是一条样本在时间上的平均：

$ hat(m)_x^((N)) = 1/N sum_(n=0)^(N-1) x[n] $

考虑原始定义，即时间平均（样本均值）等于总体平均：

$ lim_(N -> infinity) E{abs(hat(m)_x^((N)) - m_x)^2} = 0 $

不过我们没法真的从真实分布采样再去验证这个条件，所以我们提供一个均值遍历成立的*充分必要*条件（暂不考虑证明）：

// $ lim_(N -> infinity) 1/N sum_(k=0)^(N-1) c_x (k) = 0 $
// TODO: 这是课件上给的条件，下面是 GPT 给出的条件，类似的还有相关遍历处的充要条件。关于二重和同单重的在这里是否等价我拿不准，总之先如此，后者的似乎更令人信服。

$ lim_(N -> infinity) 1/N^2 sum_(n=0)^(N-1) sum_(m=0)^(N-1) c_x (n - m) = 0 $

即自协方差函数衰减得足够快，一个偏直觉的解读是，这种情况下随机变量之间的相关性不会在时间上滞留太久，长期平均可以将前后的关联稀释掉。

还有一个*充分*条件用于简单情况下的快速判断：

$ c_x (0) < infinity quad "and" quad lim_(k -> infinity) c_x (k) = 0 $

或者更强一些的条件，自方差函数绝对可和也是均值遍历的*充分*条件：

$ sum_(k=-infinity)^infinity abs(c_x (k)) < infinity $ <equ:fun_rp_ergo_autocov_abssum>

而后是*相关遍历性*（Correlation-Ergodicity），同样先定义样本自相关函数。因为 WSS 过程的自相关函数仅与时间差有关，所有样本中所有时间差为 $k$ 的样本对都可以用来估计自相关函数的第 $k$ 项，我们直接都加起来取个平均：

$ hat(r)_x^((N)) (k) = 1/N sum_(n=0)^(N-1-k) x[n+k] x^*[n] $ <equ:fun_rp_ergo_autocor_sample>

#blockquote[
    你可能会注意到该样本自相关的定义中，总共用了 $N-k$ 组样本对相关结果的加和，但除以的却是 $N$ 而非 $N-k$。
    
    这实际上确实会导致其值的估计是有偏的，但我们这里的样本自相关函数只用于后续相关遍历性的定义中，其中 $N$ 将趋于无穷，使得 $k$ 的影响可以忽略，故方便起见直接除以 $N$ 即可。

    关于这个问题让人想起样本方差计算中除以 $N-1$ 而非除以 $N$ 的经典问题。这涉及到统计学中 "自由度" 的概念，提供一个我认为比较合理但还是有一点令人疑惑的解释：
    
    从随机分布中采样的过程视为随机性的分发，而统计学从样本估计统计量可被视为是随机性的返还。
    
    在样本均值的估计中，所有的 $N$ 个样本都来自随机采样，都对估计有相等的贡献，故加和后需要除以 $N$。
    
    而样本方差的计算中用到了样本均值而非总体均值（实际分布的均值）。若样本均值已知，我们只需要知道 $N-1$ 个样本就可以算出最后一个样本的值，所以有一个样本失去了它的随机性，即所谓的失去了一个自由度，使得只有剩余的 $N-1$ 个样本对返还随机性有贡献，故最后需要除以 $N-1$。
    
    当然我觉得这还是需要严格的推导更能令人信服。不过前述样本自相关函数中没有依赖其他估计量，不存在自由度缺失的问题。
    
    #text(fill: red, "（TODO）")或许更需要考虑的是相同时间差的样本对是否对自相关函数值具有相等的贡献（将其视为样本，采样过程是否可视为等概率）。这一点我暂时无法深入思考，故不敢断言换成 $N-k$ 之后我们就能得到无偏估计。
]

同样考虑时间平均等于总体平均得到定义：

$ lim_(N -> infinity) E{abs(hat(r)_x^((N)) (k) - r_x (k))^2} = 0 $

同样这只是最朴素的式子，我们不证明地给出实用的*充分必要*条件：

// $ forall l, lim_(N -> infinity) 1/N sum_(k=0)^(N-1) abs(r_x (k) - r_x (k + l))^2 = 0 $

$ forall k, lim_(N -> infinity) 1/N^2 sum_(n=0)^(N-1) sum_(m=0)^(N-1) abs(r_x (n - m) - r_x (n - m + k))^2 = 0 $

此外，绝对可和条件 @equ:fun_rp_ergo_autocov_abssum 也是相关遍历的*充分*条件。

== Power Spectrum

#text(fill: red, "（TODO）")补充关于能量信号、功率信号、功率谱密度（PSD）。

由 Wiener–Khinchin Theorem，宽平稳（WSS）过程的功率谱密度是其自相关函数的傅里叶变换。

// WSS 是定义 PSD 的前提

== Filtering

#text(fill: red, "（TODO）")

== Random Process and Digital Signals

我们的随机过程 $x(n)$ 是分布，而我们的数字信号 $x[n]$ 可以认为是从中采样得到的某个实现。我们为随机过程定义了均值、自相关等特征，也可以为数字信号定义均值和自相关函数等运算，我们现在希望考察的就是双方的区别与联系。

澄清这一点之后，我们就可以清晰的了解该使用什么手段借助样本估计分布特征了，注意这是我们自始至终不变的目标。我们选取一些常用角度来展开这个话题。

=== Auto-correlation Estimation with Multiple Realizations <sec:fun_rp_auto_cor_estimation_with_multiple_realizations>

如果我们有大量对随机过程的采样结果（实现），我们就可以直接通过定义（@equ:fun_rp_autocor_def）来估计其分布特征，如自相关函数。

// 那么如果相关遍历性不成立呢？这种情况下我们就不能只通过单个长时间的实现来估计总体的自相关特征了。我们只能从自相关函数的定义入手，用不同的实现来直接估计它。

对随机过程 ${x(n)}$ 进行 $N$ 次实现，每次长度为 $M$，第 $i$ 次的结果记为 $x_i [n]$，或者用向量表示（当然 $M$ 也可以是无穷大，这里设为有限长度以便展示）：

$ bold(x)_i = vec(x_i [0], x_i [1], dots.v, x_i [M-1]) $

多次采样的数据我们给它拼成矩阵形式：

$
X = [bold(x)_0 quad bold(x)_1 quad dots quad bold(x)_(N-1)]
=
mat(
    delim: "[",
    x_0 [0], x_1 [0], dots, x_(N-1) [0];
    x_0 [1], x_1 [1], dots, x_(N-1) [1];
    dots.v, dots.v, dots.down, dots.v;
    x_0 [M-1], x_1 [M-1], dots, x_(N-1) [M-1];
)
$

由定义，$r_x (k, l) = E{x(k) x^*(l)}$，这里的 $E$ 就是在总体上求期望，所以我们直接用不同实现中的 $x_i [k]$ 和 $x_i [l]$ 来估计它即可：

$ hat(r)_x (k, l) = 1/N sum_(i=0)^(N-1) x_i [k] x_i^* [l] $

我们把估计自相关函数的值也写成矩阵，其可以通过 $X$ 运算得来，从而得到简洁矩阵表达：

$
hat(bold(R))_x &:=
mat(
    delim: "[",
    column-gap: #1.0em,
    row-gap: #0.5em,
    hat(r)_x (0, 0), hat(r)_x (0, 1), dots, hat(r)_x (0, M-1);
    hat(r)_x (1, 0), hat(r)_x (1, 1), dots, hat(r)_x (1, M-1);
    dots.v, dots.v, dots.down, dots.v;
    hat(r)_x (M-1, 0), hat(r)_x (M-1, 1), dots, hat(r)_x (M-1, M-1);
) \
&=
display(1/N sum_(i=0)^(N-1)) mat(
    delim: "[",
    column-gap: #1.0em,
    row-gap: #0.5em,
    x_i [0] x^*_i [0], x_i [0] x^*_i [1], dots, x_i [0] x^*_i [M-1];
    x_i [1] x^*_i [0], x_i [1] x^*_i [1], dots, x_i [1] x^*_i [M-1];
    dots.v, dots.v, dots.down, dots.v;
    x_i [M-1] x^*_i [0], x_i [M-1] x^*_i [1], dots, x_i [M-1] x^*_i [M-1];
) \
&=
inline(
    mat(
        delim: "[",
        x_0 [0], x_1 [0], dots, x_(N-1) [0];
        x_0 [1], x_1 [1], dots, x_(N-1) [1];
        dots.v, dots.v, dots.down, dots.v;
        x_0 [M-1], x_1 [M-1], dots, x_(N-1) [M-1];
    )
    mat(
        delim: "[",
        x_0 [0], x_0 [1], dots, x_0 [M-1];
        x_1 [0], x_1 [1], dots, x_1 [M-1];
        dots.v, dots.v, dots.down, dots.v;
        x_(N-1) [0], x_(N-1) [1], dots, x_(N-1) [M-1];
    )^*
) \
&=
1/N bold(X) bold(X)^H
$ <equ:fun_rp_autocor_matrix>

=== Auto-correlation Estimation with Correlation-Ergodicity

由上节可见，样本（实现）的数量 $N$ 越大，估计就越准确。但若我们只有一条样本（$N = 1$），采用这种方法的估计就将极不精确。此时，如果随机过程遍历性成立，我们就允许使用这条样本不同时间上的信息来进行估计。

我们定义有限长数字信号 $x[n]$（长度为 $N$，注意这里的 $N$ 不是上面的实现数量）的自相关函数为：

$ R_(x x) (k) = sum_(n=0)^(N-1-k) x[n + k] x^*[n] $

这从公式定义上可以理解为是对一个信号在不同时间延迟下与自身相似程度的衡量。

实际上，只要能表达这个含义即可称为自相关，故不同的信号自相关定义有很多种，只是存在细节上的差别，例如是否除以样本数量（取了一个平均）、是延迟 $k$ 还是提前 $k$（结果相当于对称了一下）等，哪个方便用哪个即可。

这里我们选取该定义是因为 MATLAB 的互相关函数 xcorr 是按这种方式定义的，文档原文是这么写的#footnote("参考 https://nl.mathworks.com/help/matlab/ref/xcorr.html#mw_01b546db-b642-4f02-8625-16078810d80f")：

$
hat(R)_(x y) (m) = cases(
    sum_(n=0)^(N-m-1) x_(n+m) y_n^*", " &m >= 0",",
    hat(R)_(y x)^* (-m)", " &m < 0".",
)
$

回忆 @equ:fun_rp_ergo_autocor_sample 定义的样本自相关函数 $hat(r)_x^((N)) (k)$，刚巧和我们定义的信号自相关函数 $R_(x x) (k)$ 形式几乎一致，只差一个系数。

根据前文的定义，如果过程是相关遍历的，那么这个样本自相关函数 $hat(r)_x^((N)) (k)$ 就可以用来正确地估计随机分布的自相关函数 $r_x (k)$。由此，与其形式几乎一致的信号互相关函数 $hat(R)_(x y) (m)$ 也就具有同样的物理意义，即也可以用于估计 $r_x (k)$（需要有系数上的调整）。

*总结来说*，相关遍历性的成立令我们可以在实现数量不足的情况下，使用其样本的信号自相关函数来估计随机过程的自相关特征。

=== Auto-correlation Estimation (Comprehensive)

如果我们有多个的实现（样本）可用，同时遍历性还成立，那么就可以综合两种优势进行估计。

具体地，@sec:fun_rp_auto_cor_estimation_with_multiple_realizations 节中我们估计的是 $r_x (k, l)$，如果遍历性成立（平稳性自然也成立），则自相关函数只与时间差有关，那么我们就可以用所有的 $hat(r)_x (n, n+k)$（$hat(bold(R))_x$ 中处在同一条斜线上的值）来估计 $r_x (k)$。

= Digital Signal Processing

#text(fill: red, "（TODO）")主要DTFT、z变换、频域特性、稳定性、功率、能量等。

= Optimization <sec:fun_optimization>

#text(fill: red, "（TODO）")主要关于最小二乘法、拉格朗日乘子法等。

#text(fill: red, "（TODO）")要写一下复变函数变量分成自己和其共轭、求解时对共轭求偏导的原因。
